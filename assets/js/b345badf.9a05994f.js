"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6503],{7998:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>h});var s=t(5893),r=t(1151);const i={id:"results-resp-bench",sidebar_label:"Results (Resp.bench)",Title:"Performance Results (Resp.bench)"},a="Evaluating Garnet's Performance Benefits",o={id:"benchmarking/results-resp-bench",title:"Evaluating Garnet's Performance Benefits",description:"We have tested Garnet thoroughly in a variety of deployment modes:",source:"@site/docs/benchmarking/results-resp-bench.md",sourceDirName:"benchmarking",slug:"/benchmarking/results-resp-bench",permalink:"/docs/benchmarking/results-resp-bench",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/Garnet/tree/main/website/docs/benchmarking/results-resp-bench.md",tags:[],version:"current",frontMatter:{id:"results-resp-bench",sidebar_label:"Results (Resp.bench)",Title:"Performance Results (Resp.bench)"},sidebar:"garnetDocSidebar",previous:{title:"Overview",permalink:"/docs/benchmarking/overview"},next:{title:"Resp.benchmark",permalink:"/docs/benchmarking/resp-bench"}},c={},h=[{value:"Setup",id:"setup",level:2},{value:"Throughput with Client Batching",id:"throughput-with-client-batching",level:2},{value:"Throughput without Client Batching",id:"throughput-without-client-batching",level:2},{value:"Latency Measurements",id:"latency-measurements",level:3},{value:"Sorted Set Performance",id:"sorted-set-performance",level:3},{value:"HyperLogLog Performance",id:"hyperloglog-performance",level:3},{value:"Bitmap Performance",id:"bitmap-performance",level:3}];function l(e){const n={a:"a",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"evaluating-garnets-performance-benefits",children:"Evaluating Garnet's Performance Benefits"}),"\n",(0,s.jsxs)(n.p,{children:["We have tested ",(0,s.jsx)(n.strong,{children:"Garnet"})," thoroughly in a variety of deployment modes:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Same local machine for client and server"}),"\n",(0,s.jsx)(n.li,{children:"Two local machines - one client and one server"}),"\n",(0,s.jsx)(n.li,{children:"Azure Windows machines"}),"\n",(0,s.jsx)(n.li,{children:"Azure Linux machines"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Below, we focus on a selected few key results."}),"\n",(0,s.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,s.jsxs)(n.p,{children:["We provision two Azure Standard F72s v2 virtual machines (72 vcpus, 144 GiB memory each) running Linux (Ubuntu 20.04), with accelerated TCP enabled. The benefit of this SKU is that we are guaranteed not to be co-located with another VM, which will optimize the performance. One machine runs different cache-store servers, and the other is dedicated to issuing workloads. We use our benchmarking tool, called ",(0,s.jsx)(n.a,{href:"resp-bench",children:"Resp.benchmark"}),", to generate all results. We compare Garnet to the latest open-source versions of Redis (v7.2), KeyDB (v6.3.4), and Dragonfly (v6.2.11) at the time of writing. We use a uniform random distribution of keys in these experiments (Garnet\u2019s shared memory design benefits even more with skewed workloads). All data fits in memory in these experiments. The baseline systems were tuned and optimized as much as possible based on available information."]}),"\n",(0,s.jsx)(n.h2,{id:"throughput-with-client-batching",children:"Throughput with Client Batching"}),"\n",(0,s.jsx)(n.p,{children:'We use a database of 256M raw string keys and values of the form "1" => "1" and so on. There is a single server instance, with multiple clients communicating with it from the second machine. Batch size is set to 4096 ops/batch by default.'}),"\n",(0,s.jsxs)(n.p,{children:["The first figure shows data loading time with 8 client sessions. We see that ",(0,s.jsx)(n.strong,{children:"Garnet"})," can perform data loading much faster than Redis, exploiting multiple server cores for performance."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Garnet benchmarks!",src:t(1037).Z+"",title:"MSET Load",width:"563",height:"334"})}),"\n",(0,s.jsxs)(n.p,{children:["Second, we perform random GET requests. As we increase the number of client sessions, ",(0,s.jsx)(n.strong,{children:"Garnet"})," scales linearly, reaching more than 60 million ops/sec, whereas Redis being single-threaded does not scale. Interestingly, even with one client session, ",(0,s.jsx)(n.strong,{children:"Garnet"})," is"]}),"\n",(0,s.jsxs)(n.p,{children:["In summary, as ",(0,s.jsx)(n.strong,{children:"Garnet"})," is based on a scalable shared memory server architecture and superior latch-free cache-friendly technology underneath, it is able to exploit batching and multiple client sessions effectively, leading to orders-of-magnitude better performance for a given shard."]}),"\n",(0,s.jsx)(n.p,{children:"We also ran experiments varying batch size, and found that even batch sizes of < 8 elements can provide huge throughput speedups."}),"\n",(0,s.jsx)(n.h2,{id:"throughput-without-client-batching",children:"Throughput without Client Batching"}),"\n",(0,s.jsxs)(n.p,{children:["We repeat the experiment above, but disable client side batching. Each client session sends a single GET request, waits for the response, and then sends the next request. The figure shows scalability as we increase the number of client sessions. Even in this extreme case, ",(0,s.jsx)(n.strong,{children:"Garnet"})," is significantly better than Redis, thanks to a more scalable server-side networking architecture that works equally well across Windows and Linux."]}),"\n",(0,s.jsx)(n.h3,{id:"latency-measurements",children:"Latency Measurements"}),"\n",(0,s.jsx)(n.p,{children:"We next report server latency as we increase the number of clients from 1 to 120. For 32 or more clients, we issue the load from two client machines to ensure that the client does not become the bottleneck. All latencies are shown in microseconds. There is no batching, and we issue the PING command."}),"\n",(0,s.jsxs)(n.p,{children:["Overall, we see that both median and 99th percentile of latency are superior with ",(0,s.jsx)(n.strong,{children:"Garnet"}),", as compared to Redis. With 120 client connections to one server:"]}),"\n",(0,s.jsxs)(n.p,{children:["The median latency of ",(0,s.jsx)(n.strong,{children:"Garnet"})," is 4X lower than Redis; and\nThe 99th percentile latency of ",(0,s.jsx)(n.strong,{children:"Garnet"})," is 3.1X lower than Redis.\nSingle thread Latency on Linux using Accelerated Networking"]}),"\n",(0,s.jsxs)(n.p,{children:["We also measured single thread latency for varying payload sizes on Linux using accelerated networking TCP and eRPC with DPDK. Below are the numbers acquired from these experiments. As shown below, ",(0,s.jsx)(n.strong,{children:"Garnet"}),"'s performance is bottlenecked by TCP. However, when using eRPC latency drops by a factor of 6x on average for the 99th percentile."]}),"\n",(0,s.jsx)(n.h3,{id:"sorted-set-performance",children:"Sorted Set Performance"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Garnet"})," supports data structures such as Sorted Set, List, Hash, and Set. These are implemented using C# collections, coupled with Tsavorite as the hash index. Here, we evaluate the performance of Sorted Set (in terms of processing ZADD and ZREM pairs), over a database of 2048 Sorted Set instances."]}),"\n",(0,s.jsxs)(n.p,{children:["To the right, we show results for batching and no-batching, as we vary the number of client sessions. As seen, ",(0,s.jsx)(n.strong,{children:"Garnet"})," offers a significant performance improvement compared to the baseline of Redis."]}),"\n",(0,s.jsx)(n.h3,{id:"hyperloglog-performance",children:"HyperLogLog Performance"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Garnet"})," supports its own built-in Hyperloglog (HLL) data structure. This is implemented using C# and it supports operations such as update (PFADD), compute an estimate (PFCOUNT) and merge (PFMERGE) two or more distinct HLL structures."]}),"\n",(0,s.jsxs)(n.p,{children:["As we expect updating the HLL data structure to be the most critical operation, we present on the right, the performance numbers for PFADD with and without batching. As indicated, ",(0,s.jsx)(n.strong,{children:"Garnet"})," offers a significant performance improvement compared to the Redis baseline."]}),"\n",(0,s.jsx)(n.h3,{id:"bitmap-performance",children:"Bitmap Performance"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Garnet"})," supports operations on Bitmap data structures. Having support for a diverse collections of commands (i.e. SETBIT,GETBIT, BITCOUNT, BITPOS, BITOP, BITFIELD), users can operate on a defined Bitmap both at a bit and byte level. Bitmaps are implemented a native type in ",(0,s.jsx)(n.strong,{children:"Garnet"})," using C# and can have an arbitrary size with maximum being 128MB."]}),"\n",(0,s.jsx)(n.p,{children:"SETBIT and GETBIT allows setting and getting individual bits of a Bitmap and BITFIELD allows for operating on groups of bits specified by the user type field."}),"\n",(0,s.jsx)(n.p,{children:"BITPOS operates at the byte level enabling discovery of the first position of the bit having been set or cleared depending on the provided value. BITCOUNT allows for counting the number of bits set in a bitmap. Both commands take as parameters a start and end byte offset within the bitmap to determine the range in which the operation will be performed."}),"\n",(0,s.jsx)(n.p,{children:"BITFIELD enables users to GET,SET and INCR any group of bits by specifying the specific type. For example we can operate on 6 bit signed integer values by using the type i6 or an unsigned 3bit value by using u3. Other parameters (e.g. OVERFLOW SAT) determine the behavior of increasing the value at a given offset by handling overflows and underflows appropriately."}),"\n",(0,s.jsxs)(n.p,{children:["It is expected that these commands are commonly executed in batches in applications where thousands of keys are accessed simultaneously. Therefore, on the right we present a sample of our evaluation measuring the throughput for all of the aforementioned operations for increasing number of client threads. As shown in the figures, ",(0,s.jsx)(n.strong,{children:"Garnet"})," significantly outperforms the Redis baseline."]})]})}function d(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},1037:(e,n,t)=>{t.d(n,{Z:()=>s});const s=t.p+"assets/images/3612712037image-7cc8776934e126d840cb5765b804ec68.png"},1151:(e,n,t)=>{t.d(n,{Z:()=>o,a:()=>a});var s=t(7294);const r={},i=s.createContext(r);function a(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);