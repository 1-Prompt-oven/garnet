"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6503],{7998:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>l});var s=t(5893),r=t(1151);const i={id:"results-resp-bench",sidebar_label:"Results (Resp.bench)",Title:"Performance Results (Resp.bench)"},a="Evaluating Garnet's Performance Benefits",o={id:"benchmarking/results-resp-bench",title:"Evaluating Garnet's Performance Benefits",description:"We have tested Garnet thoroughly in a variety of deployment modes:",source:"@site/docs/benchmarking/results-resp-bench.md",sourceDirName:"benchmarking",slug:"/benchmarking/results-resp-bench",permalink:"/docs/benchmarking/results-resp-bench",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/garnet/tree/main/website/docs/benchmarking/results-resp-bench.md",tags:[],version:"current",frontMatter:{id:"results-resp-bench",sidebar_label:"Results (Resp.bench)",Title:"Performance Results (Resp.bench)"},sidebar:"garnetDocSidebar",previous:{title:"Overview",permalink:"/docs/benchmarking/overview"},next:{title:"Resp.benchmark",permalink:"/docs/benchmarking/resp-bench"}},c={},l=[{value:"Setup",id:"setup",level:2},{value:"Basic Commands Performance",id:"basic-commands-performance",level:2},{value:"Throughput GET",id:"throughput-get",level:4},{value:"Latency GET/SET",id:"latency-getset",level:4},{value:"Complex Data Structures Performance",id:"complex-data-structures-performance",level:2},{value:"Hyperloglog",id:"hyperloglog",level:4},{value:"Bitmap",id:"bitmap",level:4},{value:"Sorted Set",id:"sorted-set",level:4},{value:"TLS Performance",id:"tls-performance",level:2},{value:"Data Ingestion",id:"data-ingestion",level:2}];function h(e){const n={a:"a",h1:"h1",h2:"h2",h4:"h4",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"evaluating-garnets-performance-benefits",children:"Evaluating Garnet's Performance Benefits"}),"\n",(0,s.jsxs)(n.p,{children:["We have tested ",(0,s.jsx)(n.strong,{children:"Garnet"})," thoroughly in a variety of deployment modes:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Same local machine for client and server"}),"\n",(0,s.jsx)(n.li,{children:"Two local machines - one client and one server"}),"\n",(0,s.jsx)(n.li,{children:"Azure Windows machines"}),"\n",(0,s.jsx)(n.li,{children:"Azure Linux machines"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Below, we focus on a selected few key results."}),"\n",(0,s.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,s.jsxs)(n.p,{children:["We provision two Azure Standard F72s v2 virtual machines (72 vcpus, 144 GiB memory each) running Linux (Ubuntu 20.04), with accelerated TCP enabled. The benefit of this SKU is that we are guaranteed not to be co-located with another VM, which will optimize the performance. One machine runs different cache-store servers, and the other is dedicated to issuing workloads. We use our benchmarking tool, called ",(0,s.jsx)(n.a,{href:"resp-bench",children:"Resp.benchmark"}),", to generate all results. We compare Garnet to the latest open-source versions of Redis (v7.2), KeyDB (v6.3.4), and Dragonfly (v6.2.11) at the time of writing. We use a uniform random distribution of keys in these experiments (Garnet\u2019s shared memory design benefits even more with skewed workloads). All data fits in memory in these experiments. The baseline systems were tuned and optimized as much as possible based on available information."]}),"\n",(0,s.jsx)(n.h2,{id:"basic-commands-performance",children:"Basic Commands Performance"}),"\n",(0,s.jsx)(n.p,{children:"We measured throughput and latency for basic GET/SET operations by varying payload size, batch size, and number of client threads.\nFor our throughput experiments, we preload a small DB (1024 keys) and a large DB (256M keys) into Garnet before running the actual workload.\nIn contrast, our latency experiments were performed on an empty database and for a combined workload of GET/SET commands that operate on a small keyspace (1024 keys)."}),"\n",(0,s.jsx)(n.h4,{id:"throughput-get",children:"Throughput GET"}),"\n",(0,s.jsxs)(n.p,{children:["For the experiment depicted in Figure 1., we used large batches of GET operations (4096 requests per batch) and small payloads (8-byte keys and values) to minimize network overhead.\nAs we increase the number of client sesssions, we observe that ",(0,s.jsx)(n.strong,{children:"Garnet"})," exhibits better scalability than Redis or KeyDB.\nDragonFly exhibits similar scaling scharacteristics though only up to 16 threads. Note also, that DragonFly is a pure in-memory system.\nOverall, ",(0,s.jsx)(n.strong,{children:"Garnet"}),"'s throughput relative to the other systems is consistently higher even when the database size (i.e., the number of distinct keys pre-loaded) is larger (at 256 million keys) than the size of the processor cache."]}),"\n",(0,s.jsxs)("figure",{children:[(0,s.jsx)("img",{src:"../../static/img/tpt-get-threads.png"}),(0,s.jsx)("figcaption",{children:"Figure 1: Throughput (log-scale), varying number of client sessions, for a database size of (a) 1024 keys, and (b) 256 million keys"})]}),"\n",(0,s.jsxs)(n.p,{children:["Even for small batch sizes ",(0,s.jsx)(n.strong,{children:"Garnet"})," outperforms the competing systems by attaining a consistently a higher throughput, as indicated by Figure 2.\nThis happens irrespective of the actual database size."]}),"\n",(0,s.jsxs)("figure",{children:[(0,s.jsx)("img",{src:"../../static/img/tpt-get-batchsize.png"}),(0,s.jsx)("figcaption",{children:"Figure 2: Throughput (log-scale), varying batch sizes, for a database size of (a) 1024 keys, and (b) 256 million keys"})]}),"\n",(0,s.jsx)(n.h4,{id:"latency-getset",children:"Latency GET/SET"}),"\n",(0,s.jsxs)(n.p,{children:["We next measure client-side latency for various systems, by issuing a mixture of 80% GET and 20% SET requests, and compare it against ",(0,s.jsx)(n.strong,{children:"Garnet"}),".\nSince we care about latency, our DB size is kept small while we vary other parameters of the workload such as client threads, batch size, and payload size."]}),"\n",(0,s.jsx)(n.p,{children:"Figure 3, showcase that as increase the number of client sessions, Garnet's latency (measured in microseconds) and across various percentiles is consistently lower and more stable compared to other systems. Note this experiment does not utilize batching."}),"\n",(0,s.jsxs)("figure",{children:[(0,s.jsx)("img",{src:"../../static/img/lat-get-set-threads.png"}),(0,s.jsx)("figcaption",{children:"Figure 3: Latency, varying number of client sessions, at (a) median, (b) 99th percentile, and (c) 99.9th percentile"})]}),"\n",(0,s.jsxs)(n.p,{children:["Garnet\u2019s latency is fine-tuned for adaptive client-side batching and efficiently handling multiple sessions querying the system.\nFor our next set of experiments, we increase the batch sizes from 1 to 64 and plot latency at different percentiles below with 128 active client connections.\nAs illustrated in Figure 4, ",(0,s.jsx)(n.strong,{children:"Garnet"})," maintains stability and achieves lower overall latency compared to other systems when the batch size is increased."]}),"\n",(0,s.jsxs)("figure",{children:[(0,s.jsx)("img",{src:"../../static/img/lat-get-set-batchsize.png"}),(0,s.jsx)("figcaption",{children:"Figure 4: Latency, varying batch sizes, at (a) median, (b) 99th percentile, and (c) 99.9th percentile"})]}),"\n",(0,s.jsx)(n.h2,{id:"complex-data-structures-performance",children:"Complex Data Structures Performance"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Garnet"})," supports a vast number of different complex data structures such as Hyperloglog, Bitmap, Sorted Sets, Lists etc.\nBelow, we present performance metrics for a select few of them."]}),"\n",(0,s.jsx)(n.h4,{id:"hyperloglog",children:"Hyperloglog"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Garnet"})," supports its own built-in Hyperloglog (HLL) data structure.\nThis is implemented using C# and it supports operations such as update (PFADD), compute an estimate (PFCOUNT) and merge (PFMERGE) two or more distinct HLL structures.\nHLL data structures are often optimized in terms of their memory footprint.\nOur implementation is no different, utilizing a sparse representation when the number of nonzero counts is low and a dense representation beyond a given fixed threshold for which\nthe trade-off between memory savings and the additional work performed for decompression is no longer attractive.\nEnabling efficient updates to the HyperLogLog (HLL) structure is essential for concurrent systems, such as Garnet.\nFor this reason, our experiments forcus specifically on the performance of PFADD and are deliberately designed to stress test our system for the following scenarios:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Large number of high contention updates (i.e. batchsize 4096, DB of 1024 keys) for increasing number of threads or increasing payload size\nAfter a few insertions, the constructed HyperLogLog (HLL) structures will transition to utilizing the dense representation."}),"\n",(0,s.jsx)(n.li,{children:"Large number of low contention updates (i.e. batchsize 4096, DB of 256M keys) for increasing number of threads or increasing payload size\nThis adjustment will increase the likelihood that the constructed HyperLogLog (HLL) structures utilize the sparse representation.\nConsequently, our measurements will consider the added overhead of working with compressed data or incrementally allocating more space for non-zero values"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["In Figure 5, we present the results for first experimental scenario.\n",(0,s.jsx)(n.strong,{children:"Garnet"})," scales very well under high contention and consistently outperforms every other system in terms of raw throughput for increasing number of threads.\nSimilarly, for increasing payload size ",(0,s.jsx)(n.strong,{children:"Garnet"})," exhibits higher total throughput compared to other systems.\nAcross all tested systems, we noticed a noticeable decrease in throughput as the payload size increased.\nThis behavior is anticipated due to the inherent TCP network bottleneck."]}),"\n",(0,s.jsxs)("figure",{children:[(0,s.jsx)("img",{src:"../../static/img/tpt-pfadd-few-keys.png"}),(0,s.jsx)("figcaption",{children:"Figure 5: Throughput (log-scale), for (a) increasing number of client sessions, and (b) increasing payload size, for a database size of 1024 keys."})]}),"\n",(0,s.jsx)(n.h4,{id:"bitmap",children:"Bitmap"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Garnet"})," supports a set of bit-oriented operators on string data types.\nThese operators can be processed in constant time (i.e. GETBIT, SETBIT) or linear time (i.e. BITCOUNT, BITPOS, BITOP).\nTo speedup processing, for the linear time operators, we used hardware and SIMD instructions.\nBelow we present the benchmark results for a subset of these operators, covering both complexity categories.\nSimilarly to before we use a small DB size (1024 keys) to evaluate the performance of each system under high contention while avoiding\nhaving all the data resident in CPU cache by increasing the paylaod size (1MB)."]}),"\n",(0,s.jsxs)(n.p,{children:["In Figure 7, we present the performance metrics for GETBIT and SETBIT commands.\nIn both cases,  ",(0,s.jsx)(n.strong,{children:"Garnet"})," consistently maintains higher throughput and better scalability as the number of client sessions increase."]}),"\n",(0,s.jsxs)("figure",{children:[(0,s.jsx)("img",{src:"../../static/img/tpt-getbit-setbit-threads.png"}),(0,s.jsx)("figcaption",{children:"Figure 7: Throughput (log-scale), varying number of client sessions, for a database size of 1024 keys and 1MB payloads."})]}),"\n",(0,s.jsxs)(n.p,{children:["In Figure 8, we evaluate the performance of BITOP NOT and BITOP AND (with two source keys) for increasing number of threads and a payload size of 1MB.\n",(0,s.jsx)(n.strong,{children:"Garnet"})," maintains overall higher throughput as they number of clieant session increase compared to every other system we tested.\nIt all also performs very well under high contention given that our DB size is relatively small (i.e. only 1024 keys)."]}),"\n",(0,s.jsxs)("figure",{children:[(0,s.jsx)("img",{src:"../../static/img/tpt-bitop-threads.png"}),(0,s.jsx)("figcaption",{children:"Figure 8: Throughput (log-scale), varying number of client sessions, for a database size of 1024 keys and 1MB payloads."})]}),"\n",(0,s.jsx)(n.h4,{id:"sorted-set",children:"Sorted Set"}),"\n",(0,s.jsx)(n.p,{children:"Sorted set performance"}),"\n",(0,s.jsx)(n.h2,{id:"tls-performance",children:"TLS Performance"}),"\n",(0,s.jsx)(n.p,{children:"TLS performance"}),"\n",(0,s.jsx)(n.h2,{id:"data-ingestion",children:"Data Ingestion"}),"\n",(0,s.jsx)(n.p,{children:"Data ingestion"})]})}function d(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>o,a:()=>a});var s=t(7294);const r={},i=s.createContext(r);function a(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);